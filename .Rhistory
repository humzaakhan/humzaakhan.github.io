•
•## Research Interests
# install and load pacman for package management
if (!require("pacman", character.only = TRUE)) install.packages("pacman")
library(pacman)
# tweets that mention "boycott"
tweets_boycott <- read.csv("https://shawnnstewart.github.io/test_data/tweets_boycott.csv?raw=true")
# tweets that mention "boycott"
tweets_boycott <- read.csv("https://shawnnstewart.github.io/test_data/tweets_boycott.csv?raw=true")
# NRC lexicon
nrc <- get_sentiments("nrc") #from tidytext
tweets <- tweets_boycott %>%
mutate(text = gsub("http\\S+", "", text)) %>%  # remove URLs
mutate(text = gsub("@\\S+", "", text)) %>% # remove mentions
filter(lang == "en") # keep only English tweets, note for boycott it's already filtered
# load libraries using pacman
p_load("Hmisc",
"tidyverse",
"tidytext",
"XML",
"wordcloud",
"RColorBrewer",
"NLP",
"tm",
"quanteda",
"quanteda.textstats" ,
"rtweet",
"igraph",
"ggraph",
"reshape2",
"ggridges",
"lubridate",
"maps",
"syuzhet",
"textdata",
"easypackages",
"boot",
"kknn",
"caret",
"leaps",
"stargazer",
"corrplot",
"xtable")
# tweets that mention "boycott"
tweets_boycott <- read.csv("https://shawnnstewart.github.io/test_data/tweets_boycott.csv?raw=true")
# tweets that mention "boycott"
tweets_boycott <- read.csv("https://shawnnstewart.github.io/test_data/tweets_boycott.csv?raw=true")
# NRC lexicon
nrc <- get_sentiments("nrc") #from tidytext
tweets <- tweets_boycott %>%
mutate(text = gsub("http\\S+", "", text)) %>%  # remove URLs
mutate(text = gsub("@\\S+", "", text)) %>% # remove mentions
filter(lang == "en") # keep only English tweets, note for boycott it's already filtered
# unnest your tweets so that there is one word per line
tweets_tidy <- tweets |>
unnest_tokens(word, text)
# unnest your tweets so that there is one word per line
tweets_tidy <- tweets |>
unnest_tokens(word, text)
tweets_tidy <- tweets_tidy |>
anti_join(stop_words)
addtl_stop_words <- c("fifa",
"fifaworldcup",
"qatar",
"qatarworldcup2022",
"boycottqatar2022",
"boycottqatar",
"worldcupqatar",
"worldcupqatar2022",
"qatarworldcup",
"qatarworldcup2022",
"worldcup2022",
"world",
"cup",
"rt",
"2022",
"qatar2022",
"fifaworldcup",
"fifaworldcup2022",
"2",
"quatar",
"quatarworldcup2022",
"boycottquatar2022",
"boycottquatar",
"worldcupquatar",
"worldcupquatar2022",
"quatarworldcup",
"quatarworldcup2022",
"quatar2022"
)
custom_stop_words <- bind_rows(
tibble(word = addtl_stop_words,
lexicon = c("custom")),
stop_words)
tweets_tidy <- tweets_tidy |>
anti_join(custom_stop_words)
nrcjoy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
# This creates a new column called "sentiment" that has a cell value of "joy" or "NA". We can turn this into a binary variable instead.
tweets_joy <- tweets_tidy %>%
full_join(nrcjoy) |>
rename(joy = sentiment) |>
mutate(joy = ifelse(joy != "", 1, 0))
# first, get the list of emotions/polarities we want to split by
emotions <- distinct(nrc, sentiment)
emo_column <- function(df, emo, lexicon = "nrc"){
# we are first going to extract just those sentiment words from lexicon
# let's give our subset a name
lexicon_subset <- paste0(lexicon,"_", emo)
lexicon_subset <- get_sentiments(lexicon) %>%
filter(sentiment == emo)
df <- df %>%
full_join(lexicon_subset) |>
rename({{emo}} := sentiment) |>
mutate(!!emo := ifelse(is.na(!!sym(emo)), 0, 1))
return(df)
}
# get list of emotions from the nrc lexicon
emotions_list <- unique(nrc$sentiment)
for(emotion in emotions_list){
tweets_tidy <- emo_column(tweets_tidy, emotion)
}
# get sums of emotion words grouped by tweet
# we also add a column with the cleaned tweet text
tweets_tidy_sum <- tweets_tidy |>
group_by(id, author_id, conversation_id) |>
dplyr::summarize(negative = sum(negative),
positive = sum(positive),
fear = sum(fear),
sadness = sum(sadness),
anger = sum(anger),
disgust = sum(disgust),
trust = sum(trust),
surprise = sum(surprise),
joy = sum(joy),
anticipation = sum(anticipation),
text = paste(word, collapse=" ")
)
#Summary of the Text: mean for negative and positive are almost the same
#words associated with trust, mean .401, are most common in the 8 emotions
#words associated with surprise, mean 0.1149, are least common in the 8 emotions
summary(tweets_tidy_sum)
#
pos <- sort(tweets_tidy_sum$positive, decreasing = TRUE)
summary(pos)
barplot(pos)
#
neg <- tweets_tidy_sum$negative
summary(neg)
barplot(neg)
nohandles <- str_replace_all(tweets$text, "@\\w+", "")
wordCorpus <- Corpus(VectorSource(nohandles))
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("amp"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("amp"))
wordCorpus <- tm_map(wordCorpus, stripWhitespace)
# prepare wordcloud
wordcloud(words = wordCorpus,
scale=c(5,0.1),
max.words=1000,
random.order=FALSE,
rot.per=0.35,
use.r.layout=FALSE)
nohandles <- str_replace_all(tweets$text, "@\\w+", "")
wordCorpus <- Corpus(VectorSource(nohandles))
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("amp"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("amp"))
wordCorpus <- tm_map(wordCorpus, stripWhitespace)
wordCorpus <- tm_map(wordCorpus, removeWords, c("qatar"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("qatar"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("qatarworldcup"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("boycottqatar"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("fifaworldcup"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("qatarworldcup2022"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("qatar2022"))
wordCorpus <- tm_map(wordCorpus, removeWords, c("world"))
# prepare wordcloud
wordcloud(words = wordCorpus,
scale=c(5,0.1),
max.words=1000,
random.order=FALSE,
rot.per=0.35,
use.r.layout=FALSE)
# Using some of our earlier versions of code to create the prepared data for the term document matrix
# let's use our tidy data to put the tweets back together, minus the stop words
tweets_tidy_cluster <- tweets_tidy |>
group_by(id, author_id, conversation_id) |>
summarize(text = paste(word, collapse=" "))
# Using some of our earlier versions of code to create the prepared data for the term document matrix
# let's use our tidy data to put the tweets back together, minus the stop words
tweets_tidy_cluster <- tweets_tidy |>
group_by(id, author_id, conversation_id) |>
summarize(text = paste(word, collapse=" "))
# Create vector
words.vec <- VectorSource(tweets_tidy_cluster$text)
# Create vector
words.vec <- VectorSource(tweets_tidy_cluster$text)
wordCorpus <- Corpus(words.vec)
# computer term-document matrix
twtrTermDocMatrix <- TermDocumentMatrix(wordCorpus)
# computer term-document matrix
twtrTermDocMatrix <- TermDocumentMatrix(wordCorpus)
twtrTermDocMatrix2 <- removeSparseTerms(twtrTermDocMatrix,
sparse = 0.97)
tweet_matrix <- as.matrix(twtrTermDocMatrix2)
# prepare distance matrix
distMatrix <- dist(scale(tweet_matrix))
# perform hierarchical clustering
fit <- hclust(distMatrix,method="single")
# perform hierarchical clustering
fit <- hclust(distMatrix,method="single")
# plot the dendrogram
plot(fit)
plot(fit, hang = -1, cex = 0.6)
plot(fit, type = "rectangle", ylab = "Height")
ch
head(ch, 3)
# Vectorize ch
words.vec <- VectorSource(ch)
# Check the class of words.vec
class(words.vec)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Install the easypackages package
install.packages("easypackages")
library(easypackages)
# Load multiple packages using easypackage function "packages"
packages("XML","wordcloud","RColorBrewer","NLP","tm","quanteda", prompt = T)
# Download text data from website
chLocation <-URLencode("http://www.historyplace.com/speeches/churchill-hour.htm)"
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Vectorize ch
words.vec <- VectorSource(ch)
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
# use htmlTreeParse function to read and parse paragraphs
doc.html<- htmlTreeParse(chLocation, useInternal=TRUE)
# Vectorize mlk
words.vec <- VectorSource(ch)
# Check the class of words.vec
class(words.vec)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Vectorize mlk
words.vec <- VectorSource(ch)
# Vectorize mlk
words.vec <- VectorSource(ch)
# Vectorize mlk
words.vec <- VectorSource(ch)
# Vectorize mlk
words.vec <- VectorSource(ch)
# Vectorize mlk
words.vec <- VectorSource(ch)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Remove punctuations, numbers
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matrix
tdm <- TermDocumentMatrix(words.corpus)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matrix
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matrix
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matrix
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
#clear memory
rm(list=ls())
# Install the easypackages package
# install.packages("easypackages")
library(easypackages)
packages("XML","wordcloud","RColorBrewer","NLP","tm","quanteda", prompt = T)
# Download text data from website
chLocation <-URLencode("http://www.historyplace.com/speeches/churchill-hour.htm")
chLocation <-URLencode("http://www.historyplace.com/speeches/churchill-hour.htm")
doc.html<- htmlTreeParse(chLocation, useInternal=TRUE)
#
ch <- unlist(xpathApply(doc.html, '//p', xmlValue))
ch
#
head(ch, 3)
# Vectorize mlk
words.vec <- VectorSource(ch)
# Check the class of words.vec
class(words.vec)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Remove punctuations, numbers
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matrix
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
head(wordCounts)
# Create Wordcloud
cloudFrame <- data.frame(word=names(wordCounts),freq=wordCounts)
set.seed(1234)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,"Dark2"))
